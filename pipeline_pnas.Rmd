---
title: Bioinformatic Pipeline - "Hiding in Plain Sight - Genome-wide recombination
  and horizontal gene transfer drive clonal diversity in Fusarium oxysporum fsp ciceris"
author: "Fayyaz A, Robinson G, Chang PL, Bekele D, Yimer SM, Carrasquilla-Garcia N, Negash K, Anand K. Surendrarao, Eric J.B. von Wettberg, Robbertse B, Seid Ahmedi, Tesfaye K, Fikre Ab, Farmer AD and Cook DR"
#date: '2023-02-07'
output:
  #rmdformats::readthedown:
  #  toc_depth: 5
  #  self_contained: true
  #  thumbnails: false
  #  lightbox: true
  #  gallery: true
  #  highlight: tango
  #html_document: default
  #pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(reticulate)
```

---

## Contents

### 1. [Overview](#Overview)
### 2. [Genome Assembly](#assembly)
### 3. [Gene Annotation and Analyses](#gene_annotation)
### 4. [Identification of SNP and Genetic Groups](#read_mapping)
### 5. [Population Analyses](#pop_gen)
### 6. [References](#refs)

---

# <a id="Overview"></a>1. Overview

![](pipeline_pnas_overview.png)

---

# <a id="assembly"></a>2. Genome Assembly

Tools required:
- Singularity v3.5.2
- FastQC v0.11.9
- Trimmomatic v0.36
- ALLPATHS-LG v52488
- A5
- FCG-GX
- BUSCO v3

### A. Quality check with FastQC

```{bash, eval=F}
module load fastqc
fastqc $IN.fastq.gz
```

### B. Low quality reads and adapters trimmed using Trimmomatic v36

```{bash, eval=F}
mkdir results/trimmomatic/"$SAMPLE"

java -jar bin/Trimmomatic-0.36/trimmomatic-0.36.jar PE -phred33 \
data/atsede-redo-libs/"$SAMPLE"_L006_R1_001.fastq.gz \
data/atsede-redo-libs/"$SAMPLE"_L006_R2_001.fastq.gz \
results/trimmomatic/"$SAMPLE"/"$SAMPLE".R1.paired.fastq.gz \
results/trimmomatic/"$SAMPLE"/"$SAMPLE".R1.unpaired.fastq.gz \
results/trimmomatic/"$SAMPLE"/"$SAMPLE".R2.paired.fastq.gz \
results/trimmomatic/"$SAMPLE"/"$SAMPLE".R2.unpaired.fastq.gz \
ILLUMINACLIP:bin/Trimmomatic-0.36/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

```

### C. Error correction using ALLPATHS-LG

```{bash, eval=F}
#ALLPATHS-LG was run with default parameters
#e.g.
ErrorCorrectReads.pl read_p1.fastq.gz read_p2.fastq.gz PHRED_ENCODING=33 READS_OUT=$OUT
```

### D. Assembly using A5

```{bash, eval=F}
#A5 was run with default parameters
#e.g.
a5_pipeline.pl read_p1.fastq.gz read_p2.fastq.gz mygenome
```

### E. Contaminant identification using FCS-GX

To identify and remove contaminants from assemblies, we used NCBI's FCS-GX (available at https://github.com/ncbi/fcs)
This program screens for contaminants by identifying contigs/scaffolds that do not originate from the biological source organism (as defined by Refseq)

```{bash, eval=F}
#load singularity
module load singularity
module load python3/3.7.4

#gathering fcs-gx singularity image
curl -LO https://github.com/ncbi/fcs/raw/main/dist/run_fcsgx.py
curl -LO https://github.com/ncbi/fcs/raw/main/examples/fcsgx_test.fa.gz
curl https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/FCS/releases/0.3.0/fcs-gx.0.3.0.sif -Lo fcsgx.sif

#create temporary shared memory
SHM_LOC=/home/melland/cook/long_read/PB852_4plex_Fusarium_HiFiv3/r64069_20230119_233108/A1/DemuxBarcodes/outputs/assemblies_hifiasm/contaminant_removal
mkdir -p "${SHM_LOC}/gxdb"
mkdir -p ./gxdb
mkdir -p ./gx_out

#verify functionality using small test-only database
#python3 ./run_fcsgx.py --fasta ./fcsgx_test.fa.gz --out-dir ./gx_out/ --gx-db "${SHM_LOC}/gxdb/test-only" --gx-db-disk ./gxdb --split-fasta --tax-id 6973 --container-engine=singularity --image=fcsgx.sif

#do full contamination analysis
#uses manifest file containing list of all fasta files paths
python3 ./run_fcsgx.py --fasta ./manifest_fo.mft --out-dir ./gx_out/ --gx-db "${SHM_LOC}/gxdb/all" --split-fasta --tax-id 5507 --container-engine=singularity --image=fcsgx.sif

# all contaminant contigs/scaffolds were removed unless they contained a bonafide Fusarium oxysporum gene (>95% identity)
```


### F. Genome completeness assessed using BUSCO v3 (Ascomycota odb9 dataset)

```{bash, eval=F}
## parameters to use for running BUSCO
docker run -it --rm -v $(pwd):/home/working -w /home/working chrishah/busco-docker run_BUSCO.py --in ./Fusarium_oxysporum.fasta --out Fusairum-BUSCO -l ./sordariomyceta_odb9 --mode genome --sp fusarium_graminearum --c 20
```

---

# <a id="gene_annotation"></a>3. Gene Annotation and Analyses

Tools required:
- Pyani v0.2.9
- RepeatModeler v1.0.11
- MAKER v2.31
- Interproscan v5.46
- BLASTp v2.7.1
- AHRD v2.0
- BLAST2GO GUI (https://www.blast2go.com/)
- Orthofinder v2.5.4

### A. Average nucleotide identity calculated with Pyani

```{bash, eval=F}
/home/afayyaz/bin/singularity run /home/afayyaz/singularityImages/average_nucleotide_identity-v0.2.9.simg -i all-genomes/ -g -m ANIb --workers 20 -o pyani-results
```


### B. Gene Annotation

#### i. The general gene annotation was done By using MAKER, RepeatModeler, Interproscan, BlastP and AHRD

RepeatModeler: We generated the library for custom repeats using RepeatModeler and used these custom library of repeats to feed into MAKER

```{bash, eval=F}
## to generate library for custom repeats using RepeatModeler

 $:/home/localhost/adf/sw/bin/singularity shell /erdos/adf/sw/singularityImages/repeatmodeler-1.0.11--pl5.22.0_0.img

./dfam-tetools.sh --singularity --container=dfam/tetools:latest --trf_prgm=/erdos/adf/sw/trf409.linux64 -- BuildDatabase -name fusariumdatabase -engine ncbi Fo-Et-0090.fasta
./dfam-tetools.sh --singularity --container=dfam/tetools:latest --trf_prgm=/erdos/adf/sw/trf409.linux64 -- RepeatModeler -database fusariumdatabase -LTRStruct -pa 20 >& repeats.out

##structural annotation using MAKER

i=`ls *.fasta`
for j in $i; do  maker -g $j 2>&1 | tee round1_$j.log ; done
mkdir snap
cd snap
maker2zff -n ../Fo-Et-0090.all.gff
fathom genome.ann genome.dna -categorize 1000
fathom uni.ann uni.dna -export 1000 -plus
forge export.ann export.dna
hmm-assembler.pl Fo-Et-0090 . > Fo-Et-0090.hmm
##used this hmm file to train next round of MAKER for all of the genome
for j in $i; do  maker -g $j 2>&1 | tee round1_$j.log ; done

##functional annotation using iprscan, blastp and AHRD for example using one genome "Fo-Et-0000"

handle_stop_codons.pl --stop_char '\*' Fo-Et-0000.all.maker.proteins.fasta > Fo-Et-0000.all.maker.proteins-stop-remove.fasta
mkdir chunks
fasta2chunks.pl --chunksize=1000 --chunk_prefix=chunks/chunk Fo-Et-0000.all.maker.proteins-stop-remove.fasta
IPRSCAN_HELPERS=/home/afayyaz/chado_preprocessing
JOB_LIMIT=32
export ANALYSIS_DATA_ROOT=`pwd`
mkdir iprscan; pushd iprscan
if [[ `which qsub` != "" ]]; then
for f in ../chunks/*; do echo ${IPRSCAN_HELPERS}/run_iprscan.bash $f | qsub -cwd -pe smp 8; done
else
for f in ../chunks/*; do
while (( `jobs | wc -l` >= $JOB_LIMIT )); do sleep 5; done;
${IPRSCAN_HELPERS}/run_iprscan.bash $f &
done
fi
#then to get iprscan "raw" output for AHRD
${IPRSCAN_HELPERS}/iprscan_convert.bash
mkdir ../iprscan_raw
mv *raw ../iprscan_raw
cd ../
mkdir ipr2go
for f in iprscan_raw/*; do ${IPRSCAN_HELPERS}/ipr2go.pl $f > `echo $f | sed 's/^iprscan_raw/ipr2go/'`; done
forkjobs_blastp_ahrd.pl --forks $JOB_LIMIT chunks blastp

perl -i ahrd_untruncate_blast_query_ids.pl blastp/fol/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/yeast/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/gram/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/vene/*
perl -i ahrd_untruncate_blast_query_ids.pl blastp/poae/*
mkdir AHRD
perl -p -e 's/\$\{([^}]+)\}/defined $ENV{$1} ? $ENV{$1} : $&/eg' /home/afayyaz/batcher_input_example_template.yml > AHRD/batcher_input_example.yml
ln -s ${INTERPRO_DB_ROOT}/interpro.dtd AHRD/interpro.dtd
pushd AHRD
mkdir batch_yml
java -cp ${AHRD_GIT_ROOT}/bin/ahrd.jar ahrd.controller.Batcher batcher_input_example.yml
perl -pi -e 's/$/ &/;' batcher.bash
source batcher.bash
for f in *csv; do if [[ ! -s all.ahrd.tsv ]]; then grep '^Protein-Accession' $f > all.ahrd.tsv; fi; sed -n '4,$p' $f >> all.ahrd.tsv; done
${IPRSCAN_HELPERS}/clean_AHRD.sh all.ahrd.tsv | sed 's/-mRNA-1//' > all.ahrd.tsv.cleaned
popd
cat iprscan_raw/* > all.iprscan_raw
awk 'BEGIN {FS="\t"} NF==9 && $2 == "maker" {print}' Fo-Et-0000.all.gff > Fo-Et-0000.all.maker.gff

##We added the annotation into gff files
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img ipr_update_gff Fo-Et-0000.all.maker.gff all.iprscan_raw > Fo-Et-0000.all.gff.iprscan_updated
${IPRSCAN_HELPERS}/add_note_attr_inGFF.pl AHRD/all.ahrd.tsv.cleaned Fo-Et-0000.all.gff.iprscan_updated > Fo-Et-0000.all.gff.iprscan_updated+AHRD

##We generated the gene id's and replace it with existing gene identifiers in the fasta and gff files.
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img maker_map_ids --prefix FO0000- Fo-Et-0000.all.gff > map
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_gff_ids map Fo-Et-0000.all.gff
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_fasta_ids map Fo-Et-0000.all.maker.proteins.fasta 
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_fasta_ids map Fo-Et-0000.all.maker.transcripts.fasta
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_gff_ids map Fo-Et-0000.all.gff.iprscan_updated+AHRD
singularity exec --bind `pwd`:/pwd --pwd /pwd /erdos/adf/sw/singularityImages/maker-2.31.9--3.img map_data_ids map AHRD/all.ahrd.tsv.cleaned
cut -d'|' -f1,1 Fo-Et-0000.all.maker.proteins.fasta | sed 's!-RA!!g' >> Fo-Et-0000.all.maker.proteins.final.fasta
cut -d'|' -f1,1 Fo-Et-0000.all.maker.transcripts.fasta | sed 's!-RA!!g' >> Fo-Et-0000.all.maker.transcripts.final.fasta
/home/afayyaz/ahrdify_fasta.pl AHRD/all.ahrd.tsv.cleaned Fo-Et-0000.all.maker.proteins.fasta Fo-Et-0000.all.maker.transcripts.fasta

##Repeated the same for rest of the genomes
```
  
#### ii. Gene Enrichment analysus was performed using BLAST2GO
  
Implemented using GUI software (https://www.blast2go.com/)
  
#### iii. SIX genes homologs were identified using BLASTp

We used an E-value cutoff of 1E-6 to ensure that they were true homologs

#### iv. Orthologs were identified with Orthofinder
```{bash, eval=F}
 singularity exec /home/afayyaz/singularity-images/orthofinder_2.5.4.sif orthofinder -t 50 -a 10 -f /home/afayyaz/99-isolate-orth-run/ > ortho_log_file 2>&1 
```
  
### C. Phylogenetic Analyses

Tools Required:
- BEAST v1.10.4
- ETE3

Phylogenetic analysis used in the manuscript are performed using BEAST

```{bash, eval=F}
##Firstly, We extracted the single copy gene names/list form BUSCO results
python3.6 find_single_copy_genes.py run_*/full_table* > busco_find_conserved_single_copy_across_all.txt
##then we extracted the fasta sequences from the genoem files
for i in `cat busco_find_conserved_single_copy_across_all.txt`; do grep -A1 $i all-busco-single-copy-genes.fna --no-group-separator >> $i.fna; done
##We used clustalo for alignment of the 1556 genes
/home/afayyaz/bin/clustalo -i genes.fna -o genes-aligned.fasta --outfmt=fasta --threads 40
##then we used the aligned file, convert it into nexus file by using seqmagick(python package) and concatenate all of the nexus file into one file for beauti software 
python3.8 seqmagick.py convert --output-format nexus --alphabet dna /home/afayyaz/aligned-genes-fasta/EOG***.fna /home/afayyaz/edit-genes/EOG***.nex
python3.8 cat-nex.py
##generated the xml file in beauti and used in Beast 
java -jar /home/afayyaz/bin/BEASTv1.10.4/lib/beast.jar -threads 20 gene_alignment.xml

##used standard parameters for all of the phylogenetic trees and visulaized in iTOL
```
  
To check the congruence between different trees, we used Robinson-Foulds with ETE3 toolkit

```{bash, eval=F}

##We called SNP's on 1556 BUSCO single copy genes using snp-sites(https://github.com/sanger-pathogens/snp-sites)
for i in `ls *.fasta`; do snp-sites -rmv -o $i $i; done

##We removed all entries with missing data
cat *fasta.vcf > all_genes.fasta.vcf 
$awk '$0 !~ "_" {print}' all_genes.fasta.vcf > all_gene_snp_nomissing.vcf 
cat all_gene_snp_nomissing.vcf | awk -v OFS="\t" '$0 !~ "^#" {hom_REF = 0; hom_ALT = 0; ; hom_var_2 = 0; hom_var_3 = 0; het = 0; for(i=10;i<=NF;i++) { if($i ~ /0/) hom_REF++; else if($i ~ /1/) hom_ALT++; else if($i ~ /2/) hom_var_2++;else if($i ~ /3/) hom_var_3++; else het++; } print $1, $2,$3, $4, $5, hom_REF, hom_ALT, hom_var_2, hom_var_3, het}' >> maf-column.txt
paste all_gene_snp_nomissing.vcf maf-column.txt > all_gene_snp-maf-file.vcf

###fixed SNP's
for F in *.vcf.gz; do bgzip -dc $F | //
awk '{for(i=11;i<=NF;i++)if($i!=$(i-1)&&$1!~/#/)next}1' | //
bgzip -c > ${F}_filtered.vcf.gz; done 

##R code for selecting SNP's
library(dplyr)
file <- read.delim("all_gene_snp-maf-file.vcf.txt")
mydata <- data.frame(file)
data <- arrange(mydata, desc(maf))
subset1 <- subset(data, maf < 0.1 )
subset2 <- subset(data, maf >= 0.1 & maf < 0.2)
subset3 <- subset(data, maf >= 0.2 & maf < 0.3)
subset4 <- subset(data, maf >= 0.3 & maf < 0.4)
subset5 <- subset(data, maf > 0.4)
###Total number of SNP's are 706149
###No of SNP's in subset1 -> 299273
###No of SNP's in subset2 -> 402709
###No of SNP's in subset3 -> 2103
###No of SNP's in subset4 -> 1052
###No of SNP's in subset5 -> 1012
##We randomly selected snps from subsets based on the percentage representation in the total amount

x <- slice_sample(subset1, n=126831)
y <- slice_sample(subset2, n=229624) 
z <- slice_sample(subset3, n=6)
a <- slice_sample(subset4, n=1)
b <- slice_sample(subset5, n=1)

#randomly selecting snps from subsets based on the percentage representation in the total amount
write.table(x, file = "x-export-file.txt", sep = "\t" )
write.table(y, file = "y-export-file.txt", sep = "\t" )
write.table(z, file = "z-export-file.txt", sep = "\t" )
write.table(a, file = "a-export-file.txt", sep = "\t" )
write.table(b, file = "b-export-file.txt", sep = "\t" )
#concatente in bash and add header of the original vcf file.

#For fixed SNP's

###Total number of SNP's are 613071
###No of SNP's in subset1 -> 607962
###No of SNP's in subset2 -> 3486
###No of SNP's in subset3 -> 795
###No of SNP's in subset4 -> 455
###No of SNP's in subset5 -> 329

x <- slice_sample(subset1, n=601882)
y <- slice_sample(subset2, n=20) 
z <- slice_sample(subset3, n=1)
a <- slice_sample(subset4, n=1)
b <- slice_sample(subset5, n=1)
write.table(x, file = "x-export-file.txt", sep = "\t" )
write.table(y, file = "y-export-file.txt", sep = "\t" )
write.table(z, file = "z-export-file.txt", sep = "\t" )
write.table(a, file = "a-export-file.txt", sep = "\t" )
write.table(b, file = "b-export-file.txt", sep = "\t" )
#concatente in bash and add header of the original vcf file.

##converted vcf format to fasta format and used 
cat subset.vcf | perl vcf_to_fasta.pl > vcftofasta.fasta
##generated the xml file in beauti and used in Beast 
java -jar /home/afayyaz/bin/BEASTv1.10.4/lib/beast.jar -threads 20 gene_alignment.xml
##generated 10 different trees newick files labeld as "1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt"
ete3 compare -taboutput -t 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt -r 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt 10.txt > result.txt
```
  
### D. <a id="pangenome"></a>Pangenome

Tools required:
- PanOCT v3.23
- VCFtools v0.1.15

Pangenome was constructed using PanOCT software with percentage identity threshold 95%. We used PanOCT because not only does it identify close orthologs but accounts for synteny during cluster prediction

```{bash, eval=F}
singularity exec /home/afayyaz/singularity-images/panoct_3.23--pl526_1.sif panoct.pl -t blast-results.txt -g final-attribute.txt -P 99-all-protein.pep -f list.txt -b /home/afayyaz/panoct/ -i 95 -L 20 -M Y -F 1.33 -N Y -H Y -V Y -S Y -C Y -B Y -c 0,5,25,50,75,90,95,98,100 > panoct-log 2>&1
```

We used upset plot to demonstrate how many accessory genes lineages O, P, R, and E shared with each other

```{r}
## R Markdown
install.packages("UpSetR")
install.packages("VennDiagram")
suppressWarnings(suppressMessages(library(UpSetR)))
suppressWarnings(suppressMessages(library(VennDiagram)))
suppressWarnings(suppressMessages(library(RColorBrewer)))
input1 <- c(
  "E&O" = 958,
"E&P" = 1126,
"E&R" = 955,
"O&P"= 2796,
"O&R" = 2545,
"P&R" = 2774,
"E&O&P" = 647,
"E&O&R" = 564,
"E&P&R" = 643,
"O&P&R" = 1959,
"E&O&P&R" = 403
)

upset(fromExpression(input1), 
      nintersects = 40, 
      nsets = 4, 
      order.by = "freq", 
      decreasing = T, 
      mb.ratio = c(0.6, 0.4),
      number.angles = 0, 
      text.scale = 1.1, 
      point.size = 2.8, 
      line.size = 1
      )
```
  
Before we calculated the nucleotide diversity for the pangenome, we removed the paralogs from the gene presence absence file
 
```{bash, eval=F}
###Removed paralogs from panoct result file
Python3 remove-paralogs.py
###To calculate nucleotide diversity for pan-genome, we needed the files of each cluster in the pangenome containing number of genomes from 5 to 99
awk -F"\t" '{print >> ($101".txt")}' without-paralogs.txt  
###Then we made a list of the cluster in the pangenome
for i in `ls cluster*`; do sed $'s/\t/\\\n/g' $i| sed '/^$/d' | sed '$d' | sed '$d' > $i.txt; done
###We extracted the cluster sequence from each genome
for i in `ls cluster*.txt`; do python3 extract_seq.py final-99-isolate-transcript.fasta $i $i.fasta; done
###We aligned the sequence using mafft
for i in `ls *.fasta`; do mafft --thread 20 $i > $i.aligned.fasta; done
###We called SNP's using python package snp-sites(https://github.com/sanger-pathogens/snp-sites)
for i in `ls *.fasta`; do snp-sites -rmv -o $i $i; done
for i in `ls *.vcf`; do sed 's/*/_/g' $i | awk '$0 !~ "_" {print}' | awk '{for(i=1;i<=NF;i++){if($i==0){$i="0/0"}}}{$1=$1} 1' OFS="\t" | awk '{for(i=10;i<=NF;i++){if($i==1){$i="1/1"}}}{$1=$1} 1' OFS="\t"| awk '{for(i=10;i<=NF;i++){if($i==2){$i="2/2"}}}{$1=$1} 1' OFS="\t" > $i-p.vcf; done
###We calculated nucleotide diversity using vcftools
for i in `ls *-p.vcf`; do vcftools --vcf $i --site-pi --out $i-nucl_diversity; done
for i in `ls *.pi`; do sed '1d' $i > $i.txt ; done
##We took the average of each file
ls *.txt > file_list
while read line; do awk '{sum+=$2} END {print sum/NR}' $line >> avg-pi-file; done<list-avg-pi.txt
mv file_new avg-values
cp *-aligned-files/avg* .
for i in `ls avg-*`; do awk '{print FILENAME (NF?"\t":"") $0}' $i > $i.txt; done
cat *.txt > all-cluster-pi.txt
###We calculated tajimas-d using vcftools
for i in `ls *-p.vcf`; do vcftools --vcf $i --TajimaD 100000000 --out $i-tajimad.txt; done
for i in `ls *.Tajima.D.txt`; do sed '1d' $i> $i.txt; done
##We took the average of each file
while read line; do awk '{sum+=$5} END {print sum/NR}' $line >> avg-tajima-file; done<list-avg-tajima.txt
mv file_new avg-values
cp *-aligned-files/avg* .
for i in `ls avg-*`; do awk '{print FILENAME (NF?"\t":"") $0}' $i > $i.txt; done
cat *.D.txt > tajima.d.txt
##repeat these steps for all clusters containing number of genomes from 5 to 99
```
  
---

# <a id="read_mapping"></a>4. Identification of SNP and genetic groups

### A. Read mapping and Variant calling

Tools required:
- BWA MEM v0.7.9a-r786
- SAMtools v1.3.1
- VCFtools v0.1.15
- BCFtools v1.16
- Picard 
- GATK v4.1

```{bash, eval=F}
#create index for F. oxysporum reference genome (Fol4287 - GCF_000149955.1)
bwa index [-a bwtsw|is] reference.fasta index_prefix
samtools faidx reference.fasta
##created sequence dictionary
java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar CreateSequenceDictionary REFERENCE=reference.fasta OUTPUT=reference.dict
##lign reads and assign read group
for i in $(ls *.fastq.gz | cut -d '_' -f1,2,3| uniq); do bwa mem -t 60 -R "@RG\tID:${i}\tSM:${i}\tPL:illumina\tPU:Lane1\tLB:${i}" reference.fasta ${i}_R1_pair_001.fastq.gz ${i}_R2_pair_001.fastq.gz > ${i}.sam; done
##sorting sam files
for i in `ls *.sam`; do java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar SortSam I=$i O=$i.bam SORT_ORDER=coordinate; done
##mark duplicates
for i in `ls *.bam`; do java -jar /home/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar MarkDuplicates I=$i O=$i-edit.bam REMOVE_DUPLICATES=true METRICS_FILE=$i-metrics.txt; done
##sort bam file
for i in `ls *-edit.bam`; do java -jar /erdos/cook_lab/afayyaz/software/picard/build/libs/picard-2.25.7-SNAPSHOT-all.jar BuildBamIndex INPUT=$i ; done
##called variants
for i in `ls *-edit.bam`; do gatk HaplotypeCaller -R reference.fasta -I $i -ploidy 1 --native-pair-hmm-threads 40 -ERC GVCF -O $i-raw_gVCF.vcf --spark-runner LOCAL; done
for i in `ls *-raw_gVCF.vcf`; do gatk GenotypeGVCFs -O $i-raw.vcf -R reference.fasta --spark-runner LOCAL -V $i; done
for i in `ls *-raw.vcf`; do bgzip -c $i > $i.gz; done
for i in `ls *-raw.vcf.gz`; do tabix -p vcf $i ; done
bcftools merge *vcf.gz -o a-merged.vcf
##extract SNP's and stats
gatk SelectVariants -R reference.fasta -V a-merged.vcf -select-type SNP -O c-snps.vcf
bcftools stats c-snps.vcf > stats-snps.txt
vcftools --vcf c-snps.vcf --out c-filtered --minDP 10 --max-missing 0.95 --minQ 30 --recode --recode-INFO-all
```


### B. Filtering VCF file 

Tools required:
- BCFtools v1.16
- VCFtools v0.1.15
- MUMmer v3.23

For most population analyses, we exclusively used high quality genomes (based on BUSCO completeness > 99%, N50, number of scaffolds and genome size) - see "Genome Assembly". This provided a total list of 120 genomes, 99 of which were taxonomically within the FOSC (Fig. 2). This is reduced from the earlier VCF file, which contained 320 isolates (Supplemental Fig. 1)

Reducing the VCF file from 320 to 120, was done using bcftools (custom bash script).

#### i. We split the VCF into separate single isolate VCF files

```{bash, eval=F}
######## ./splitting_vcfs_retaining.sh ###########

#!/usr/bin/env bash
for file in *.vcf.gz; do
  for sample in `bcftools query -l $file`; do
    bcftools view -Oz -s $sample -o ${file/.vcf*/.$sample.vcf.gz} $file
  done
done
```

#### ii. We indexed all the single isolate VCF files

```{bash, eval=F}
for f in *vcf.gz; do tabix -f -p vcf $f; done
```

#### iii. We merged isolates from list of high quality genomes (information obtained from assembly data)

```{bash, eval=F}
bcftools merge --file-list isolate_list > ethiopia_fo_structure.vcf
```

#### iv. We removed heterozygous/ambiguous SNPs using sed

```{bash, eval=F}
sed -i 's#0/1|0/2|0/3|0/4|1/2|1/3|1/4|2/3|2/4|3/4#[.]/[.]#g' ethiopia_fo_structure.vcf
```

#### v. We removed SNPs with high missingness and evaluated MAF filtering

To ensure that the subsequent VCF was high quality, especially for linkage disequilibrium, we adjusted filters to learn how different filters affect the subsequent linkage disequilibrium analyses. 

The main two parameters were missingness and minor allele frequency (MAF). MAF was of particular importance to us, as often researchers adjust MAF cutoff as >0.05 without regard for the effects on the population. 

```{bash, eval=F}
#VCF file for population genetic analyses
vcftools --vcf ethiopia_fo_structure.vcf --max-missing 0.9 --recode --stdout | bgzip -c > all_ethiopia_isolates.vcf.gz #total number of SNPs = 136772
```

This filtering step was evaluated to understand the effects of using MAF cutoffs (no MAF filter vs "typical" MAF >0.05). There were only minor differences in the outcomes. The clonal groupings remained the same. The LD decay curve only showed minor differences (see below)

![](ld_total_maf.png)

#### vi. To ensure SNPs were core components of the genome, mapping was informed by additional whole genome alignments (MUMmer)

This was done on per chromosome basis (Fol4287 1-15)

```{bash, eval=F}
#Here is the example of chromosome 7 (Suppl. Fig. 2)
for f in Fo-Et-0*; do nucmer -maxmatch -c 100 -p ${f}_chr7 chr7.fna ${f}; done #outputs delta file

# Whole genome alignments were visualized and filtered in R
Rscript ./mummer_coverage.R

# This led to the removal of any SNPs mapped to chromosomes 3, 6, 14 and 15 as well as supercontig 27 (end of chromosome 1)
# Any reference contigs that mapped poorly to the Illumina genomes were removed running awk itieratively
# e.g. awk '($0 !~ "Supercontig_2.27"){print}' all_ethiopia_isolates.vcf.gz
```


### C. Identification of clonal groups using STRUCTURE

Tools required:
- STRUCTURE v2.3.4
- Structure Harvester (https://taylor0.biology.ucla.edu/structureHarvester/)
- CLUMPP v1.1.2

Subdividing clonal lineages in fungal populations remains challenging. Traditionally, reasearchers have determined clonality based on identical MLGs. This approach fails on a whole genome scale due to the accumulation of erroneous mutations. We employed two different methods to subdivide the lineages: phylogenetics (see Gene Annotation and Analyses - BEAST) and K clustering (via STRUCTURE). 

#### i. <a id="struct"></a>Random subsampling SNPs

To avoid SNP selection bias, the VCF file was randomly subsampled at a rate of ~10%. 
Note that this script does not provide exactly 10% of the total number of SNPs (Table 1 - Non-clone corrected = 136)

```{bash, eval=F}

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub1.vcf.gz

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub2.vcf.gz

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub3.vcf.gz

bgzip -dc all_ethiopia_isolates.vcf.gz | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .1) print $0}' | bgzip -c > ethiopia_fo_structure.0_05.vcf.gz_sub4.vcf.gz

#etc.
```

VCF subsample files were converted into genotype files using a manual bash script

```{bash, eval=F}
############ ./convertvcftogeno.sh ############
#!/usr/bin/env bash
file=$1
out=${file}.out
bgzip -dc ${file} | awk '$1 ~ "Super" || $1 ~ "#CHROM" {print}' | awk '$1 !~ "##" {print}' | sed 's#0/0#0#g;s#1/1#1#g;s#2/2#2#g;s#3/3#3#g;s#4/4#4#g;s#0/1#-9#g;s#0/2#-9#g;s#0/3#-9#g;s#0/4#-9#g;s#1/2#-9#g;s#1/3#-9#g;s#1/4#-9#g;s#2/3#-9#g;s#2/4#-9#g;s#3/4#-9#g;s#./.#-9#g' | cut -f 2,10- | awk '
{ 
    for (i=1; i<=NF; i++)  {
        a[NR,i] = $i
    }
}
NF>p { p = NF }
END {    
    for(j=1; j<=p; j++) {
        str=a[1,j]
        for(i=2; i<=NR; i++){
            str=str" "a[i,j];
        }
        print str
    }
}' > $out
```

#### ii. STRUCTURE
  
Ran STRUCTURE on each subsampling 10 times: burnin = 10,000 and MCMC reps = 50,000. K vals = 2-20. Check out Mainparams and Extraparams for additional technical information.
  
```{bash, eval=F}
# e.g.
#!/bin/bash
module load structure-console/2.3.4
structure -K 10 -o structure_large10-1-2
```

#### iii. STRUCTUREharvester

Using outputs of STRUCTURE, combine them using STRUCTURE_HARVESTER (online web server: http://taylor0.biology.ucla.edu/structureHarvester/) to produce .indfiles for CLUMPP.

#### iv. CLUMPP

Ran CLUMPP on HPC cluster to collate 10 permutations of each subsample. Used LargeKGreedy algorithm.

```{bash, eval=F}
#e.g.
#!/bin/bash
module load CLUMPP/1.1.2
CLUMPP K2.paramfile -i K12.indfile -k 12 -c 133 -r 10 -m 3 -w 0 -s 2 -o K12.outfile -j K12.miscfile
```

#### v. STRUCTURE plotted with R script

```{bash, eval=F}
######### ./structure_plotting_larger_collection #############
```

#### vi. Clonal groups were assigned based on Euclidean distances

```{bash, eval=F}
######### ./plotting_structure_network.R ##########
./plotting_structure_network.R --list "list of all .outfile paths" --label "list of isolate names (in order)" --title "title of graph (usually K val included)"
```

---

# <a id="pop_gen"></a>5. Population Analyses

### A. Geographic Analyses

#### i. Plotting onto maps in R

To create plots of geographic sampling (both chickpea and Fungal sampling)

```{bash, eval=F}
######### ./geo_plot_ethiopia.R ###########
```

![](pipeline_figure1.gif)
![](pipeline_geog_subgroups.gif) 

![](pipeline_geog_mat.png)

#### ii. Mantel tests

The Mantel test is a measure of correlation between two distance matrices. Here we want to understand how genetic difference (both sequence and presence/absence variation) is correlated with geography.
  
```{r, eval=F}
##We prepared the co-ordinates for all infected sample into data frame for mental test
library(ade4)
location = read.delim("location-for-distance-plot-99-isolates.txt")
rownames(location) <- location[, "Sample"]
location <- location[, -1]
x = as.matrix(location)
y <- distm(x, fun = distGeo)
physical_distance <- melt(y)

n <- max(table(physical_distance$Sample)) + 1  # +1,  so we have diagonal of 
res <- lapply(with(physical_distance, split(value, physical_distance$Sample)), function(x) c(rep(NA, n - length(x)), x))
res <- do.call("rbind", res)
res <- rbind(res, rep(NA, n))
res <- as.dist(t(res))
res
##mantel test for accessary genome usimg Jaccarad distance
m <- max(table(pan_distance$X)) + 1  # +1,  so we have diagonal of 
res2 <- lapply(with(pan_distance, split(value, pan_distance$X)), function(x) c(rep(NA, m - length(x)), x))
res2 <- do.call("rbind", res2)
res2 <- rbind(res2, rep(NA, m))
res2 <- as.dist(t(res2))
res2
mantel.rtest(res, res2, nrepet = 9999)
###manetl test for core using Conserved Ortholog genes were also performed the same.
```
  
  
### B. Linkage disequilibrium

We calculated linkage disequilibrium using two different methods:

1. Pearson's correlation coefficient (r2) [implemented in PLINK]
2. Index of association (rbarD) [implemented in PoppR]

#### i. Linkage Disequilibrium via r2

Tools required:
- Plink v1.90
- BCFtools v1.16

#### Preparing datasets for "clonally-derived" vs "ancestrally-derived" SNPs (Figure 4 and Table 1)

"Clonally-derived" SNPs are variants that are variable within clonal subgroups. 

"Ancestrally-derived" SNPs are variants that are fixed within clonal subgroups 

To do this, we separated the final VCF file (See Filtering VCF) into the subgroups defined by BEAST (phylogenetics) and STRUCTURE (K clustering).

1. Split the VCf file into groups

```{bash, eval=F}
######## ./splitting_vcfs_retaining.sh ###########

#!/usr/bin/env bash
for file in *.vcf.gz; do
  for sample in `bcftools query -l $file`; do
    bcftools view -Oz -s $sample -o ${file/.vcf*/.$sample.vcf.gz} $file
  done
done
```

2. Combine into VCF files on a clade basis using BCFtools

```{bash, eval=F}
#e.g. This is an example for clonal subgroup E. Note that this was done for all groups E-U
#note that "clade_e.txt" is a text file containing the list of isolates contained in subgroup E
bcftools merge -l clade_e.txt > clade_e.vcf.gz
```

3. Filtering step for clonally-derived SNPs
```{bash, eval=F}
bgzip -dc  $group.vcf.gz | // #input file lineages
awk '($0 ~ "1/1" && $0 ~ "0/0") || ($0 ~ "1/1" && $0 ~ "2/2") || //
($0 ~ "2/2" && $0 ~ "0/0") || ($0 ~ "3/3" && $0 ~ "0/0") || //
($0 ~ "4/4" && $0 ~ "0/0") || ($0 ~ "3/3" && $0 ~ "1/1") || //
($0 ~ "4/4" && $0 ~ "1/1") || ($0 ~ "3/3" && $0 ~ "2/2") || //
($0 ~ "4/4" && $0 ~ "2/2") || ($0 ~ "3/3" && $0 ~ "4/4") || //
$1 ~ /#/ {print $0}' | bgzip -c > $output.vcf.gz #output SNPs variable across each clonal group
#note that these files were used to generate Figure 4C (decay plot for each clonal lineage)

bcftools merge -l $(ls clade*) > var_clones.vcf.gz
```

4. Filtering step for ancestrally-derived SNPs

```{bash, eval=F}
#filter across clonal subgroup
for F in *.vcf.gz; do bgzip -dc $F | //
awk '{for(i=11;i<=NF;i++)if($i!=$(i-1)&&$1!~/#/)next}1' | //
bgzip -c > ${F}_filtered.vcf.gz; done #output SNPs fixed across entire clonal group

#choose one sequence from the subgroup VCF files
for F in *_filtered.vcf.gz; do bgzip -dc $F | //
cut -f 1-11 | //
bgzip -c > ${F}_represent.vcf.gz; done

#merge representative isolates
bcftools merge -l $(ls *_represent.vcf.gz) > eth_clonecorrected.vcf.gz
```

#### Linkage disequilibrium - r2

1. Run intra-chromosomal PLINK LD

```{bash, eval=F}
#code for clonally-derived SNPs
plink --vcf var_clones.vcf.gz --allow-extra-chr --r2 //
--ld-window-r2 0 --ld-window 99999 --out var_clone

#code for ancestrally-derived SNPs
plink --vcf total_clone.vcf.gz --allow-extra-chr --r2 --ld-window 99999 //
--ld-window-r2 0 --out total_clone

#note that PLINK automatically retains only biallelic 

#calculating bp difference from PLINK LD output
for f in *.ld; do awk -F' ' '{print $1, $2, $5, $7, ($2-$5) }' $f | sed 's/-//g;s/ /\t/g' > diff_calculated/$f.diff; done

#filtering for bp difference <75000 (saves memory to permit R-based analysis)
for f in *diff; do awk '($5 < 75000) {print}' $f > rm_large_diffs/$f.75000.ld; done

#rest of plotting done in R
```

2. Run inter-chromosomal PLINK LD

Since inter-chromosomal SNPs can create unreasonably large file sizes (>100GB), we subset the SNP files before running

```{bash, eval=F}
#selects ~2.5% of the total SNPs (~2000-3000 SNPs)
#run on both var_clone.vcf.gz and eth_clonecorrected.vcf.gz
awk 'BEGIN {srand()} !/^$/ { if (rand() <= .025 || FNR==1) print $0}' $IN.vcf.gz > $OUT-filtered.vcf.gz

#calculating interchromosomal LD
plink --vcf var_clone.vcf.gz-filtered.vcf.gz --allow-extra-chr --inter-chr --r2 //
--ld-window-r2 0 --out var_clone_inter

#calculating interchromosomal LD
plink --vcf eth_clonecorrected.vcf.gz-filtered.vcf.gz --allow-extra-chr --inter-chr --r2 //
--ld-window-r2 0 --out total_clone_inter

#mean of each LD was calculated (background LD)
# file.ld = any inter-chromosomal PLINK LD output
count=0; total=0; for i in $( awk '{ print $7; }' file.ld );\
do total=$(echo $total+$i | bc ); \
((count++)); done; echo "scale=2; $total / $count" | bc
```

3. Plotting r2 decay curves and genome-wide r2

r2 decay plots

```{r, eval=F}
############# ld_decay.R ##################
```

![](pipeline_ld_decay_figures.gif)

r2 genome-wide plots

```{r, eval=F}
############## making_ld_windows.Rmd ###############
```

![](pipeline_ld_across_window.gif)

#### ii. Linkage disequilibrium - rbarD

To complement the Pearson's correlation coefficient (r2) LD data, we included the index of association (rbarD)(Agapow and Burt, 2001)

One issue that we experienced with this metric though is that it is very computationally intensive (at least in PoppR). As a result, most SNP sets had to be downsampled to a size of ~3000 SNPs

We used the same subsamples as for [STRUCTURE](#struct). As a result, the rbarD calculations were averaged over the 10 subsamples.

We used PoppR (Kamvar et al, 2014) to run rbarD. The VCF files were converted into geno files using vcfR (Knaus et al, 2017)

The generalized framework for calculating the index of association (rbarD) in R:

```{r, eval=F}
##########################################################################
## For full code, see ./ia_analysis.Rmd. Here is an example of how the ###
## index of association (rbarD) was calculated using Poppr. This example #
## contains only Subsample 1. ############################################
##########################################################################

#load required libraries
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
library(vcfR)
library(adegenet)
library(poppr)
library(magrittr)

##############################################################################
#function to prep VCF data
file_prep <- function(file) {
  isolate_vcf <- read.vcfR(file = file, verbose = FALSE) #import vcf file (can also be vcf.gz)
  isolate_genind <- vcfR2genind(isolate_vcf) #converts vcf into genind format
  isolate_genind$other$lineage <- c("SING", "SING", "SING", "SING", "SING", "T", "SING", "T", "O", "SING", "Q", "Q", "O", "J", "N", "SING", "N", "M", "M", "N", "SING", "P", "SING", "K", "N", "T", "Q", "SING", "P", "T", "N", "N", "P", "J", "J", "SING", "SING", "S", "S", "SING", "O", "SING", "SING", "T", "P", "T", "P", "R", "SING", "SING", "SING", "O", "R", "SING", "SING", "U", "SING", "SING", "E", "SING", "SING", "Q", "SING", "R", "O", "Q", "R", "SING", "SING", "R", "R", "R", "SING", "Q", "SING", "SING", "R", "O", "I", "M", "R", "E", "H", "SING", "G", "H", "F", "SING", "H", "SING", "L", "L", "O", "H", "H", "P", "R", "SING", "O", "T", "K", "E", "O", "SING", "O", "SING", "F", "G", "G", "M", "O", "Q", "P", "E", "T", "K", "R", "R", "SING", "O", "I", "U", "E", "SING", "P", "P", "R", "SING", "N", "R", "O", "O", "SING") #predetermined lineages from STRUCTURE and phylogenetic analysis
  pop(isolate_genind) <- isolate_genind$other$lineage #assigns lineages as populations
  isolate_genclone <- as.genclone(isolate_genind) #creates genclone file for poppr analysis
}

###################################################################
ss1 <- file_prep("subsample_1.vcf")
#etc.
# All 10 subsamples were loaded into R like this.
# (Subsample #) : (# of SNPs)
# Subsample 1 : 3756
# Subsample 2 : 3665
# Subsample 3 : 3826
# Subsample 4 : 3653
# Subsample 5 : 3764
# Subsample 6 : 3777
# Subsample 7 : 3682
# Subsample 8 : 3744
# Subsample 9 : 3784
# Subsample 10 : 3747

###################################################################
# non-clone corrected Ia and Rbar calculations
set.seed(100)
resmpl_ia_ss1 <- resample.ia(ss1, reps = 999, n=132)
#etc.

#combine all subsampled data together
resmpl_ia_all <- rbind(resmpl_ia_ss1,
    resmpl_ia_ss2,
    resmpl_ia_ss3,
    resmpl_ia_ss4,
    resmpl_ia_ss5,
    resmpl_ia_ss6,
    resmpl_ia_ss7,
    resmpl_ia_ss8,
    resmpl_ia_ss9,
    resmpl_ia_ss10)

  write_delim(resmpl_ia_all, file = "results/all_isolates_ia_vals.txt", delim = "\t")
resmpl_ia_all <- read_delim("results/all_isolates_ia_vals.txt", delim = "\t")
std(resmpl_ia_all$rbarD)


t <- ggplot(resmpl_ia_all, aes(x = rbarD)) +
  geom_histogram(bins=100) +
  geom_vline(xintercept = ia(ss1)[2]) +
#  geom_vline(xintercept = 0.0702, linetype=2) +
  geom_vline(xintercept = ia(clonecorrect(ss1))[2], linetype = 2) +
  xlab(expression(bar(r)[d]))
t

######################################################################
#separating populations before index of association calculation
obj1 <- seppop(ss1) #separates the genclone objects into a list of different populations specified earlier
#etc.

####SS1
sing_genclone1 <- obj1$SING
e_genclone1 <- obj1$E
f_genclone1 <- obj1$F
g_genclone1 <- obj1$G
h_genclone1 <- obj1$H
i_genclone1 <- obj1$I
k_genclone1 <- obj1$K
l_genclone1 <- obj1$L
m_genclone1 <- obj1$M
n_genclone1 <- obj1$N
o_genclone1 <- obj1$O
p_genclone1 <- obj1$P
q_genclone1 <- obj1$Q
r_genclone1 <- obj1$R
s_genclone1 <- obj1$S
t_genclone1 <- obj1$T

# calculate Ia and rbarD for subgroup E, subsample 1
resmpl_ia_ss1_e <- resample.ia(e_genclone1, reps = 999, n=(nrow(as.data.frame(e_genclone1))-1))
#etc. 
# calculated for all other subsamples (E) too
#combined all subsamples for subgroup E together

resmpl_ia_e <- rbind(resmpl_ia_ss1_e,
    resmpl_ia_ss2_e,
    resmpl_ia_ss3_e,
    resmpl_ia_ss4_e,
    resmpl_ia_ss5_e,
    resmpl_ia_ss6_e,
    resmpl_ia_ss7_e,
    resmpl_ia_ss8_e,
    resmpl_ia_ss9_e,
    resmpl_ia_ss10_e)

# save data for subgroup E
write_delim(resmpl_ia_e, file = "results/E_isolates_ia_vals.txt", delim = "\t")


###### calculated for all other subgroups too #########

# Averages (Mean) and Standard Deviations were calculated in R #
```


## C. Nucleotide Diversity (pi)

Tools required:

- VCFtools v0.1.15

Nucleotide diversity (pi) was calculated from VCF files (both [read-mapping](#read_mapping) using VCFtools (Danecek et al, 2011)

```{bash, eval=F}
vcftools --gzvcf $IN.vcf.gz --window-pi  10000 --out $OUT 
```

## D. Tajima's D

Tools required:

- VCFtools v0.1.15

Tajima's D was calculated from VCF files for [read-mapping](#read_mapping) directly using VCFtools (Danecek et al, 2011). The Tajima's D analysis calculated for the pangenome required additional steps and so is described above under "[Pangenome](#pangenome)".

For the readmapping VCF files, Tajima's D was calculated per SNP. These were then averaged across the genome in R.

```{bash, eval=F}
vcftools --gzvcf $IN.vcf.gz --TajimaD  1 --out $OUT 
```


## E. Genetic Differentiation (Fst) (Weighted)

Genetic differentiation (Fst) is a commonly used proxy to determine gene flow between multiple populations, because more gene flow = lower genetic differentiation. 

We wanted to understand the levels of genetic differentiation between different clonal subgroups (and therefore estimate the level of gene flow between purely clonal lineages). The idea behind this is to understand whether gene flow is still ongoing even after a transition to clonality. As a result, the Fst of sequence variation (core SNPs) was compared to presence/absence variation. Hypothesis: If horizontal gene transfer is ongoing, we expect to see lower genetic differentiation between clonal subgroups that core SNP variation. Furthermore, they should display different demographic histories (i.e. the two Fst sets should be correlated). 

To account for genic selection, we included a supplemental figure displaying sequence variation of conserved orthologs. This should serve as a sandwich with the core gene Fst (under highest purifying selection) and core read-mapped Fst (under no selection) serving as the two extremes. If selection is the only reason for differentiation, we expect that the accessory gene variation to be sandwiched between these two extremes. 

We used custom R scripts to calculate weighted Fst values. The input files for each R script are VCF files (haplotype) or presence/absence matrix (coverage) containing only specified subgroups.

```{bash, eval=F}
#sequence variation (core genes and core SNPs)
# input is any VCF file
############# calculating_haplotype_diversity.R ##################
Rscript ./calculating_haplotype_diversity.R --populationa $clonal_lineageA.vcf --populationb $clonal_lineageB.vcf > $OUT


#presence/absence variation (accessory genes)
############# calculating_coverage_diversity.R ##################
Rscript ./calculating_coverage_diversity.R --populationa $clonal_lineageA.txt --populationb $clonal_lineageB.txt > $OUT

```

One can see that this phenomenon is not due to selection exclusively in the boxplot below

![](selection_snps_tajD.png)

---

# <a id="refs"></a>6. References
